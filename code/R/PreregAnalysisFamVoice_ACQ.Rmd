---
title: "Preregistered Analysis FamVoice ACQ"
output: html_document
date: "`r Sys.Date()`"
---

This is the analysis for the project "The Voice Familiarity Benefit for Infant Phoneme Acquisition". Please note: the variable TestSpeaker (levels: 1, 2) in this analysis document is called Test (levels: TrainVoiceTest and NovelVoiceTest) in the accompanying preregistration. TestSpeaker 1 corresponds to TrainVoiceTest, and TestSpeaker2 corresponds to NovelVoiceTest.

```{r, include=FALSE}
library(here)
library(tidyverse)
library(brms)
library(worcs)
library(ggmcmc)
library(RColorBrewer)
library(ggplot2)
library(easystats)
library(emmeans) 
library(logspline)

```
### Simulate some data
```{r, message=FALSE}
source(here("code/R","01_data_simulation.R"), local = knitr::knit_global())
```

```{r, include=TRUE}
head(dat_acq)
summary(dat_acq)
```

### Set priors based on pilot data
Load the pilot data
```{r, message=FALSE}
data_pilot = read_csv(here("data","data_pilot.csv"))
data_pilot$TestSpeaker = as.factor(data_pilot$TestSpeaker)
```

Now we have a look at the pilot data:
```{r}
plot(density(data_pilot$MMR),
     main="Pilot data",xlab="MMR")
mean(data_pilot$MMR)
sd(data_pilot$MMR)
```
We see that the data are normally distributed, with mean = 3.546836, SD = 16.40196

This means that we will choose a Gaussian function as the likelihood function for all models. We will check with our experimental data whether they are also normally distributed. If not, we will adapt the likelihood function accordingly.

Now we run a model with weakly informative priors. All the models in this R Markdown file are knit beforehand with the commented out code (for transparency), and are then read in (for efficiency).
``` {r}
prior_pilot <-
  c(
    prior("normal(0, 35)", class = "Intercept"), #  weakly informative prior on intercept 
    prior("normal(0, 35)", class = "b") #  weakly informative prior on slope
  )

# pilot_m = brm(MMR ~ 1 + TestSpeaker + (1 | Subj), 
#                prior = prior_pilot,
#                data = data_pilot,
#                control = list(
#                  adapt_delta = .99, 
#                  max_treedepth = 15),
#                file = here("data", "model_output", "02_pilot.rds")
#                )

pilot_m <- readRDS(here("data", "model_output", "02_pilot.rds"))
summary(pilot_m)
```
This model estimates a mean of 3.53, and a sigma of 16.12.

We base our priors on this:

- mean = 3.5
- sigma = 20 (We add some uncertainly, so we multiply sigma by 1.25)
- we expect our effects to be smaller than 20, so we set a prior on the slope of ```normal(0,10)```

giving us:
```{r}
priors <- c(set_prior("normal(3.5, 20)",  # sd=20 is here the SD of the distribution of the prior on mu, so how uncertain we are about the value of mu
                      class = "Intercept"),
            set_prior("normal(0, 10)",  # This is the prior on the slope
                      class = "b"),
            set_prior("normal(0, 20)",  # This is our expectation about the error in the model, so the residual noise. It's related to the overall variability in the response variable that is not captured by the predictors.
                      class = "sigma")) # brms will automatically truncate the prior specification for σ and allow only positive values (https://vasishth.github.io/bayescogsci/book/ch-reg.html), so we don't have to truncate the distribution ourselves

```

Let's visualize the priors 
```{r, echo = FALSE}
# Visualize the priors
dpri <- data.frame(x = seq(-70,70,by=1))
dpri$y1 <- dnorm(dpri$x,mean=3.5,sd=20)
dpri$y2 <- dnorm(dpri$x,mean=0,sd=10)
dpri$y3 <- dnorm(dpri$x,mean=0,sd=20)

dprig <- dpri %>% gather(y1, y2, y3, key="Prior", value="Density")
dprig$Prior <- factor(dprig$Prior)
levels(dprig$Prior) <- c("Prior for mu\ndnorm(x, mean=3.5, sd=20)", "Prior for slope\ndnorm(x, mean=0, sd=10)", "Prior for sigma\ndnorm(x, mean=0, sd=20)")
ggplot(data=dprig, aes(x=x, y=Density)) + geom_line() + facet_wrap(~Prior, scales="free")

```


### Prior predictive checks
Here, we check what happens if we run the model based on our priors only.

```{r}
# num_chains <- 4 
# num_iter <- 4000 
# num_warmup <- num_iter / 2 
# num_thin <- 1 

# priorpredcheck_acq_m <- brm(MMR ~ 1 + TestSpeaker * Group + 
#                               mumDist +
#                               nrSpeakersDaily  + 
#                               sleepState + 
#                               age +
#                               (1 + TestSpeaker | Subj),
#                             data = dat_acq,
#                             prior = priors,
#                             family = gaussian(),
#                             control = list(
#                               adapt_delta = .99, 
#                               max_treedepth = 15
#                             ),
#                             iter = num_iter, 
#                             chains = num_chains, 
#                             warmup = num_warmup,
#                             thin = num_thin,
#                             cores = num_chains, 
#                             seed = project_seed,
#                             file = here("data", "model_output", "03_model_priorpredcheck_acq.rds"),
#                             file_refit = "on_change",
#                             save_pars = save_pars(all = TRUE),
#                             sample_prior = "only"
# )

priorpredcheck_acq_m <- readRDS(here("data", "model_output", "03_model_priorpredcheck_acq.rds"))

pp <- posterior_predict(priorpredcheck_acq_m) 
pp <- t(pp)
# distribution of mean MMR
meanMMR = colMeans(pp)
hist(meanMMR, breaks = 20)
summary(priorpredcheck_acq_m)
```
In our priors, we specified a mean of 3.5 and an SD of 20. In these plots and the model summary, we see that the mean of the MMR is estimated at 3.40 and is normally distributed, with an estimated error of 20.47. Sigma was specified at ```normal(0,20)``` - the estimate of 16.05 with an estimated error of 11.87 seems reasonable. The effects on the slope were specified at ```normal(0,10)``` which also seems to be reasonably estimated. 

Conclusion: our priors seem to be reasonable.

### Posterior predictive checks
Here, we check whether our model can recover our simulated data, with our proposed priors. Our data were simulated as follows:

- mean = 5
- SD = 15
- effect of Group = 5
- effect of TestSpeaker = 5

We run the model with more iterations (20.000) to ensure full convergence. 

```{r}
num_chains <- 4 
num_iter <-20000 
num_warmup <- num_iter / 2 
num_thin <- 1 
```

We also set the contrast such that we get equal priors on the different comparisons. This is important for our Bayes Factor analysis later on. However, since we only have factors with max. 2 levels, this is the same as deviation coding with 0.5 and -0.5

```{r}
contrasts(dat_acq$TestSpeaker) <- contr.equalprior_pairs
contrasts(dat_acq$Group) <- contr.equalprior_pairs
contrasts(dat_acq$sleepState) <- contr.equalprior_pairs

contrasts(dat_acq$TestSpeaker)
contrasts(dat_acq$Group) 
contrasts(dat_acq$sleepState) 
```

Now let's look at the model:
```{r}
# postpredcheck_acq_m <- brm(MMR ~ 1 + TestSpeaker * Group + 
#                             mumDist +
#                             nrSpeakersDaily +
#                             sleepState +
#                              age +
#                              (1 + TestSpeaker | Subj),
#                            data = dat_acq,
#                            prior = priors,
#                            family = gaussian(),
#                            control = list(
#                              adapt_delta = .99, 
#                              max_treedepth = 15
#                            ),
#                            iter = num_iter, 
#                            chains = num_chains, 
#                            warmup = num_warmup,
#                            thin = num_thin,
#                            cores = num_chains, 
#                            seed = project_seed,
#                            file = here("data", "model_output", "04_model_posteriorpredcheck_acq.rds"),
#                            file_refit = "on_change",
#                            save_pars = save_pars(all = TRUE)
# )

postpredcheck_acq_m <- readRDS(here("data", "model_output", "04_model_posteriorpredcheck_acq.rds"))
summary(postpredcheck_acq_m)
```

There were 45 divergent transitions after warmup. However, the model seems to have converged, since the Rhat and ESS values are good enough.

We can now check the values the model comes up with for the simulated data: <br />
b_Intercept is 4.06 (in the data it was simulated as 5). <br />
It retrieves an effect of 4.38 for TestSpeaker. <br />
It retrieves an effect of 3.21 for Group. <br />
It also hallucinates some (small) effects for the covariates. <br />
Sigma is estimated at 11.99.


Now we check the traces and posterior distributions
```{r}
plot(postpredcheck_acq_m) # check traces and posterior distributions
```

Here we see that the traces all look like nice hairy caterpillars, meaning that the chains have mixed. The posterior distributions also look good (and we checked the estimates in the summary already).


Now we check whether the model is able to retrieve the underlying data. y is the observed data, so the data that we inputted, 
and y' is the simulated data from the posterior predictive distribution. 
```{r}
pp_check(postpredcheck_acq_m, ndraws=50)
```
This looks good: the data overlap.

It thus seems as if both our priors and our model are adequate for our simulated data. We assume that this will also be the case for our experimental data. We will run the prior predictive check also for the experimental data, and the posterior predictive checks will be repeated in our model diagnostics.

### Sensitivity analysis of the model's estimates
To check how dependent the model's estimates are on the priors, we perform a sensitivity analysis. We will check the estimates of our model with our proposed priors (intercept: ``normal(3.5,20)``, slope ```normal(0,10)```, sigma ``normal(0,20)``), and three alternative priors:

- Alternative 1: Intercept and sigma: ``normal(0,20)``, slope ```normal(0,10)```: the same SD but the a mean of zero
- Alternative 2: Intercept and sigma: ``normal(0,30)``, slope ```normal(0,20)```: weakly informative, because of the large SD
- Alternative 3: Intercept and sigma: ``normal(0,50)``, slope ```normal(0,40)```: uninformative, not really biologically plausible anymore

```{r}
priors_orig <- 
  c(set_prior("normal(3.5, 20)",  
              class = "Intercept"),
    set_prior("normal(0, 10)",  
              class = "b"),
    set_prior("normal(0, 20)",  
              class = "sigma"))

priors1 <- 
  c(set_prior("normal(0, 20)",   
              class = "Intercept"),
    set_prior("normal(0, 10)",  
              class = "b"),
    set_prior("normal(0, 20)",  
              class = "sigma"))

priors2 <-
    c(set_prior("normal(0, 30)",   
              class = "Intercept"),
    set_prior("normal(0, 20)",  
              class = "b"),
    set_prior("normal(0, 30)",  
              class = "sigma"))

priors3 <-
    c(set_prior("normal(0, 50)",
              class = "Intercept"),
    set_prior("normal(0, 40)",  
              class = "b"),
    set_prior("normal(0, 50)",  
              class = "sigma"))
```

Let's plot these different priors for the intercept:

```{r, echo =FALSE}
# set colors
col1 = "#F8766D"
col2 = "#FCAE00"
col3 = "#00BFC4"
col4 = "#C77CFF"
# Plot the priors (code adapted from https://osf.io/eyd4r/)
legend_colors <- c("Orig: normal(3.5, 20)" = col1, "Alt 1: normal(0, 20)" = col2, "Alt 2: normal(0, 30)" = col3, "Alt 3: normal(0, 50)" = col4)
# Save the x-axis limits in a dataframe
df <- data.frame(x = c(-50, 50))
p <- ggplot(df, aes(x=x)) +
  # First, add the prior distribution of the original prior. 
  # The first line creates an area (filled in with color), the second line creates a line graph
  stat_function(fun = dnorm, n = 1001, args = list(mean = 3.5, sd = sqrt(20)), geom = "area", aes(fill = "Orig: normal(3.5, 20)"), alpha = .5) +
  stat_function(fun = dnorm, n = 1001, args = list(mean = 3.5, sd = sqrt(20))) +
  # Repeat the above for each of the two alternative priors
  stat_function(fun = dnorm, n = 1001, args = list(mean = 0, sd = sqrt(20)), geom = "area", aes(fill = "Alt 1: normal(0, 20)"), alpha = .5) +
  stat_function(fun = dnorm, n = 1001, args = list(mean = 0, sd = sqrt(20))) +
  stat_function(fun = dnorm, n = 1001, args = list(mean = 0, sd = sqrt(30)), geom = "area", aes(fill = "Alt 2: normal(0, 30)"), alpha = .5) +
  stat_function(fun = dnorm, n = 1001, args = list(mean = 0, sd = sqrt(30))) +
  stat_function(fun = dnorm, n = 1001, args = list(mean = 0, sd = sqrt(50)), geom = "area", aes(fill = "Alt 3: normal(0, 50)"), alpha = .5) +
  stat_function(fun = dnorm, n = 1001, args = list(mean = 0, sd = sqrt(50))) +
  scale_fill_manual(name='Priors', values = legend_colors,
                    breaks = c("Orig: normal(3.5, 20)", "Alt 1: normal(0, 20)",
                               "Alt 2: normal(0, 30)", "Alt 3: normal(0, 50)")) +
  ggtitle("Comparison of Priors") +
  xlab(bquote(beta[Intercept])) +
  theme_light() +    
  theme(legend.position = c(0.8, 0.8),
        axis.title.x=element_text(family = "sans", size = 18),
        axis.title.y=element_blank(),
        plot.title = element_text(hjust = 0.5, family="sans", size = 18))
# Print the plot
p
```

We ran the model of the posterior checks (see above) with these three alternative priors, such that we can compare the different estimates:

```{r}
m_orig <- readRDS(here("data", "model_output", "04_model_posteriorpredcheck_acq.rds"))
m_alt_1 <- readRDS(here("data", "model_output", "05_model_sensitivity_altern1_acq.rds"))
m_alt_2 <- readRDS(here("data", "model_output", "05_model_sensitivity_altern2_acq.rds"))
m_alt_3 <- readRDS(here("data", "model_output", "05_model_sensitivity_altern3_acq.rds"))

posterior_summary(m_orig, variable=c("b_Intercept","b_TestSpeaker1", "b_Group1", "sigma"))
posterior_summary(m_alt_1, variable=c("b_Intercept","b_TestSpeaker1", "b_Group1", "sigma"))
posterior_summary(m_alt_2, variable=c("b_Intercept","b_TestSpeaker1", "b_Group1", "sigma"))
posterior_summary(m_alt_3, variable=c("b_Intercept","b_TestSpeaker1", "b_Group1", "sigma"))
```

We can now also plot different estimates for the different priors:

```{r, echo=FALSE, warning =FALSE}
# Visualize posterior distributions
m_orig_tranformed <- ggs(m_orig) # the ggs function transforms the brms output into a longformat tibble, that we can use to make different types of plots.
m_alt_1_tranformed <- ggs(m_alt_1)
m_alt_2_tranformed <- ggs(m_alt_2)
m_alt_3_tranformed <- ggs(m_alt_3)

legend_colors <- c("Original" = col1, "Alternative 1" = col2, "Alternative 2" = col3, "Alternative 3" = col4)

# plot posterior density for Intercept
ggplot() + 
  geom_density(data = filter(m_orig_tranformed,
                             Parameter == "b_Intercept", 
                             Iteration > 10000), aes(x = value, fill  = "Original"), alpha = 1) +
  geom_density(data = filter(m_alt_1_tranformed,
                             Parameter == "b_Intercept", 
                             Iteration > 10000), aes(x = value, fill  = "Alternative 1"), alpha = .5) +
  geom_density(data = filter(m_alt_2_tranformed,
                             Parameter == "b_Intercept", 
                             Iteration > 10000), aes(x = value, fill  = "Alternative 2"), alpha = .5) +
  geom_density(data = filter(m_alt_3_tranformed,
                             Parameter == "b_Intercept", 
                             Iteration > 10000), aes(x = value, fill  = "Alternative 3"),  alpha = .5) +
  scale_x_continuous(name   = "Value",
                     limits = c(-5, 15)) + 
  scale_fill_manual(name='Priors', values = legend_colors, 
                    breaks = c("Original", "Alternative 1", 
                               "Alternative 2", "Alternative 3")) +
  # set v-lines for the CIs
  geom_vline(xintercept = summary(m_orig)$fixed[1,3],
             col = col1,
             linetype = 2) +
  geom_vline(xintercept = summary(m_orig)$fixed[1,4],
             col = col1,
             linetype = 2) +
  geom_vline(xintercept = summary(m_alt_1)$fixed[1,3],
             col = col2,
             linetype = 2) +
  geom_vline(xintercept = summary(m_alt_1)$fixed[1,4],
             col = col2,
             linetype = 2) +  
  geom_vline(xintercept = summary(m_alt_2)$fixed[1,3],
             col = col3,
             linetype = 2) +
  geom_vline(xintercept = summary(m_alt_2)$fixed[1,4],
             col = col3,
             linetype = 2) +
  geom_vline(xintercept = summary(m_alt_3)$fixed[1,3],
             col = col4,
             linetype = 2) +
  geom_vline(xintercept = summary(m_alt_3)$fixed[1,4],
             col = col4,
             linetype = 2) +
  theme_light() +
  theme(legend.position = c(0.8, 0.8)) +
  labs(title = "Posterior Density of the Intercept for different priors")

# plot posterior density for effect TestSpeaker
ggplot() + 
  geom_density(data = filter(m_orig_tranformed,
                             Parameter == "b_TestSpeaker1", 
                             Iteration > 10000), aes(x = value, fill  = "Original"), alpha = .5) +
  geom_density(data = filter(m_alt_1_tranformed,
                             Parameter == "b_TestSpeaker1", 
                             Iteration > 10000), aes(x = value, fill  = "Alternative 1"), alpha = .5) +
  geom_density(data = filter(m_alt_2_tranformed,
                             Parameter == "b_TestSpeaker1", 
                             Iteration > 10000), aes(x = value, fill  = "Alternative 2"), alpha = .5) +
  geom_density(data = filter(m_alt_3_tranformed,
                             Parameter == "b_TestSpeaker1", 
                             Iteration > 10000), aes(x = value, fill  = "Alternative 3"),  alpha = .5) +
  scale_x_continuous(name   = "Value",
                     limits = c(-15, 10)) + 
  scale_fill_manual(name='Priors', values = legend_colors,
                    breaks = c("Original", "Alternative 1", 
                               "Alternative 2", "Alternative 3")) +
  #set v-lines for the CIs
  geom_vline(xintercept = summary(m_orig)$fixed[2,3],
             col = "yellow",
             linetype = 2) +
  geom_vline(xintercept = summary(m_orig)$fixed[2,4],
             col = "yellow",
             linetype = 2) +
  geom_vline(xintercept = summary(m_alt_1)$fixed[2,3],
             col = "#F8766D",
             linetype = 2) +
  geom_vline(xintercept = summary(m_alt_1)$fixed[2,4],
             col = "#F8766D",
             linetype = 2) +  
  geom_vline(xintercept = summary(m_alt_2)$fixed[2,3],
             col = "#00BA38",
             linetype = 2) +
  geom_vline(xintercept = summary(m_alt_2)$fixed[2,4],
             col = "#00BA38",
             linetype = 2) +
  geom_vline(xintercept = summary(m_alt_3)$fixed[2,3],
             col = "#619CFF",
             linetype = 2) +
  geom_vline(xintercept = summary(m_alt_3)$fixed[2,4],
             col = "#619CFF",
             linetype = 2) +
  theme_light() +
  theme(legend.position = c(0.8, 0.8)) +
  labs(title = "Posterior Density of the Effect of TestSpeaker for different priors")

# plot posterior density for effect Group
ggplot() + 
  geom_density(data = filter(m_orig_tranformed,
                             Parameter == "b_Group1", 
                             Iteration > 10000), aes(x = value, fill  = "Original"), alpha = .5) +
  geom_density(data = filter(m_alt_1_tranformed,
                             Parameter == "b_Group1", 
                             Iteration > 10000), aes(x = value, fill  = "Alternative 1"), alpha = .5) +
  geom_density(data = filter(m_alt_2_tranformed,
                             Parameter == "b_Group1", 
                             Iteration > 10000), aes(x = value, fill  = "Alternative 2"), alpha = .5) +
  geom_density(data = filter(m_alt_3_tranformed,
                             Parameter == "b_Group1", 
                             Iteration > 10000), aes(x = value, fill  = "Alternative 3"),  alpha = .5) +
  scale_x_continuous(name   = "Value",
                     limits = c(-15, 10)) + 
  scale_fill_manual(name='Priors', values = legend_colors,
                    breaks = c("Original", "Alternative 1", 
                               "Alternative 2", "Alternative 3")) +
  #set v-lines for the CIs
  geom_vline(xintercept = summary(m_orig)$fixed[3,3],
             col = "yellow",
             linetype = 2) +
  geom_vline(xintercept = summary(m_orig)$fixed[3,4],
             col = "yellow",
             linetype = 2) +
  geom_vline(xintercept = summary(m_alt_1)$fixed[3,3],
             col = "#F8766D",
             linetype = 2) +
  geom_vline(xintercept = summary(m_alt_1)$fixed[3,4],
             col = "#F8766D",
             linetype = 2) +  
  geom_vline(xintercept = summary(m_alt_2)$fixed[3,3],
             col = "#00BA38",
             linetype = 2) +
  geom_vline(xintercept = summary(m_alt_2)$fixed[3,4],
             col = "#00BA38",
             linetype = 2) +
  geom_vline(xintercept = summary(m_alt_3)$fixed[3,3],
             col = "#619CFF",
             linetype = 2) +
  geom_vline(xintercept = summary(m_alt_3)$fixed[3,4],
             col = "#619CFF",
             linetype = 2) +
  theme_light() +
  theme(legend.position = c(0.8, 0.8)) +
  labs(title = "Posterior Density of the Effect of Group for different priors")

```

The alternative models have 667, 344, and 88 divergent transitions, respectively. For all of these models, Rhat is sometimes 1.01. ESS is low for the sd's in Alt 1. The estimates, however, seem to be reliable for all models (since they are very similar).

As we can see from the (plotted) estimates, the different priors barely have any effect on the estimates of the model. This means that our model is not overly sensitive to the priors.

We will run the same sensitivity analysis for our model with our experimental data.

### Parameter estimation experimental data

For the experimental data, we will use the following code for parameter estimation, assuming the same data structure as our simulated data, and the data called ```dat_exp_acq```

```
# scaling the continuous predictors:
dat_exp_acq$mumDist<- scale(dat_exp_acq$mumDist)
dat_exp_acq$age <- scale(dat_exp_acq$age)
dat_exp_acq$nrSpeakersDaily <- scale(dat_exp_acq$nrSpeakersDaily)

# Setting orthogonal contrasts, such that the priors for the different contrasts we compare are orthogonal (not so important in this case since we only have 2 level factors)
contrasts(dat_exp_acq$TestSpeaker) <- contr.equalprior_pairs
contrasts(dat_exp_acq$Group) <- contr.equalprior_pairs
contrasts(dat_exp_acq$sleepState) <- contr.equalprior_pairs

# Setting up the samping:
num_chains <- 4 # number of chains = number of processor cores
num_iter <- 40000 # number of samples per chain: because I use Savage-Dickey for hypothesis testing, so we need a LOT of samples
num_warmup <- num_iter / 2 # number of warm-up samples per chain
num_thin <- 1 # thinning: extract one out of x samples per chain

# Running the model
model_acq <- brm(MMR ~ 1 + TestSpeaker * Group +
                   mumDist +
                   nrSpeakersDaily +
                   sleepState +
                   age +
                   (1 + TestSpeaker | Subj),
                 data = dat_exp_acq,
                 prior = priors,
                 family = gaussian(),
                 control = list(
                   adapt_delta = .99,
                   max_treedepth = 15
                 ),
                 iter = num_iter,
                 chains = num_chains,
                 warmup = num_warmup,
                 thin = num_thin,
                 cores = num_chains,
                 seed = project_seed,
                 file = here("data", "model_output", "A2_model_acq.rds"),
                 file_refit = "on_change",
                 save_pars = save_pars(all = TRUE)
)
```

### Model Diagnostics
We will then run some model diagnostics.
First, we will plot the traces, to check model convergence, with ```plot(model_acq, ask = FALSE)```. We will also check the posterior samples of the posterior predictive distribution, with
``` 
posterior_predict_model_acq <-
  model_acq %>%
  posterior_predict(ndraws = 2000)
  
PPS_acq <-
  posterior_predict_model_acq %>%
  ppc_stat_grouped(
    y = pull(dat_exp_acq, MMR),
    group = pull(dat_exp_acq, TestSpeaker_Group),
    stat = "mean"
  ) +
  ggtitle("Posterior predictive samples")
```

### Hypothesis testing
Now we want to test our hypotheses. Our main research question is: Is there a Voice Familiarity Benefit for phoneme acquisition in infants? We address three sub-questions:

1.	Do infants discriminate a vowel contrast better when they were trained on this contrast with a familiar voice?
2.	Does this voice familiarity benefit for phoneme acquisition generalize to a novel voice? That is, do infants discriminate a vowel contrast spoken by a novel voice better when they were trained on this contrast by a familiar voice?
3.	Do infants discriminate a vowel contrast better when it is spoken by the speaker by which they were trained?
4.  Is there a relationship between number of speakers exposed to in daily life and vowel discrimination?

**Hypothesis 1 (directional):** <br />
We expect that infants discriminate a vowel contrast better from the training voice when this speaker’s voice was familiar during the training, shown by a more negative mismatch response (MMR) in the FamTrain group than in the UnFamTrain group condition in the TrainVoiceTest (speaker 1). <br />
**Hypothesis 2 (non-directional):** <br />
We expect a difference between the FamTrain group and the UnFamTrain group on the NovelVoiceTest (speaker 2). <br />
**Hypothesis 3 (directional):** <br />
We expect to find that infants discriminate the vowel contrast better in the TrainVoiceTest than in the NovelVoiceTest, shown by a more negative MMR in TrainVoiceTest (speaker 1) than in the NovelVoiceTest (speaker 2), irrespective of training group.<br />
**Hypothesis 3 (non-directional):** <br />
We expect a relationship between number of speakers exposed to in daily life and vowel discrimination. This relationship could be either positive or negative (for a positive relation between speakers exposed to in daily life and vowel discrimination, we would expect a negative relationship between speakers exposed to in daily life and MMR, since we assume that the more negative the MMR is, the better the discrimination). <br />

We thus want the following comparisons
1. For TestSpeaker = 1, the MMR is more negative for group=fam vs group = unfam.<br />
2. For TestSpeaker = 2, the MMR is different for group=fam vs group = unfam.<br />
3. For both groups together: MMR is larger for TestSpeaker = 1 as for TestSpeaker = 2. <br />
4. For both groups and speakers together: we expect a correlation between MMR and nrSpeakersDaily. <br />


This means we want the following contrasts:<br />
for Group:TestSpeaker:<br />
RQ1: Groupfam TestSpeaker1 - Groupunfam TestSpeaker1. <br />
RQ2: Groupfam TestSpeaker2 - Groupunfam TestSpeaker2. <br />
for TestSpeaker:<br />
RQ3: TestSpeaker1 - TestSpeaker2<br />
general:<br />
RQ4: nrSpeakersDaily<br />

For our analyses we will use Bayes Factors. However, as a robustness check, and to be able to compare our results with other experiments that use frequentist methods, we will also compute MAP-Based p-Value (pMAP; https://easystats.github.io/bayestestR/reference/p_map.html). This is the Bayesian equivalent of the p-value, and is related to the odds that a parameter (described by its posterior distribution) has against the null hypothesis (h0) using Mills' (2014, 2017) Objective Bayesian Hypothesis Testing framework. It corresponds to the density value at 0 divided by the density at the Maximum A Posteriori (MAP).
Caveats the p_MAP:
1) p_MAP allows to assess the presence of an effect, not its *magnitude* or *importance* (https://easystats.github.io/bayestestR/articles/probability_of_direction.html)
2) p_MAP is sensitive only to the amount of evidence for the *alternative hypothesis* (i.e., when an effect is truly present). It is *not* able to reflect the amount of evidence in favor of the *null hypothesis*. A high value suggests that the effect exists, but a low value indicates uncertainty regarding its existence (but not certainty that it is non-existent) (https://doi.org/10.3389/fpsyg.2019.02767).

Bayes Factors (https://doi.org/10.3389/fpsyg.2019.02767).
The Bayes factor indicates the degree by which the mass of the posterior distribution has shifted further away from or closer to the null value (0) relative to the prior distribution, thus indicating if the null hypothesis has become less or more likely given the observed data. The BF is computed as a Savage-Dickey density ratio, which is also an approximation of a Bayes factor comparing the marginal likelihoods of the model against a model in which the tested parameter has been restricted to the point-null (Wagenmakers et al., 2010).

We will use the following code to compute the pMAP:
```
##  Effect all ------------------------
pMAP_all_acq = 
  model_acq %>%
  p_map() %>%
  mutate(
    "p < .05" = ifelse(p_MAP < .05, "*", "")
  )

## Simple Effect Group ------------------------
pMAP_Group_simple_acq = 
  model_acq %>%
  emmeans(~ Group|TestSpeaker) %>%
  pairs() %>%
  p_map() %>%
  mutate(
    "p < .05" = ifelse(p_MAP < .05, "*", "")
  )
pMAP_Group_simple_acq

##  Effect Speaker ------------------------
pMAP_Speaker_acq = 
  model_acq %>%
  emmeans(~ TestSpeaker) %>%
  pairs() %>%
  p_map() %>%
  mutate(
    "p < .05" = ifelse(p_MAP < .05, "*", "")
  )
pMAP_Speaker_acq
```

We will use the following code to compute the BFs:
```
model_acq_prior <- unupdate(model_acq) # sample priors from model

##  Effect all ------------------------
# Bayes Factors (Savage-Dickey density ratio)
BF_all_acq <-
  model_acq %>%
  bf_parameters(prior = model_acq_prior) %>%
  arrange(log_BF) # sort according to BF

# add rule-of-thumb interpretation
BF_all_acq <-
  BF_all_acq %>%
  add_column(
    "interpretation" = interpret_bf(
      BF_all_acq$log_BF,
      rules = "raftery1995",
      log = TRUE,
      include_value = TRUE,
      protect_ratio = TRUE,
      exact = TRUE
    ),
    .after = "log_BF"
  )

## Simple Effect Group ------------------------
# pairwise comparisons of prior distributions
Group_simple_pairwise_prior_acq =
  model_acq_prior %>%
  emmeans(~ Group|TestSpeaker) %>% # estimated marginal means
  pairs() # pairwise comparisons

# pairwise comparisons of posterior distributions
Group_simple_pairwise_acq <-
  model_acq %>%
  emmeans(~ Group|TestSpeaker) %>%
  pairs()

# Bayes Factors (Savage-Dickey density ratio)
BF_Group_simple_acq <-
  Group_simple_pairwise_acq %>%
  bf_parameters(prior = Group_simple_pairwise_prior_acq) %>%
  arrange(log_BF) # sort according to BF

# add rule-of-thumb interpretation
BF_Group_simple_acq <-
  BF_Group_simple_acq %>%
  add_column(
    "interpretation" = interpret_bf(
      BF_Group_simple_acq$log_BF,
      rules = "raftery1995",
      log = TRUE,
      include_value = TRUE,
      protect_ratio = TRUE,
      exact = TRUE
    ),
    .after = "log_BF"
  )

BF_Group_simple_acq

##  Effect Speaker ------------------------
# pairwise comparisons of prior distributions
Speaker_pairwise_prior_acq <-
  model_acq_prior %>%
  emmeans(~ TestSpeaker) %>% # estimated marginal means
  pairs() # pairwise comparisons

# pairwise comparisons of posterior distributions
Speaker_pairwise_acq <-
  model_acq %>%
  emmeans(~ TestSpeaker) %>%
  pairs()

# Bayes Factors (Savage-Dickey density ratio)
BF_Speaker_acq <-
  Speaker_pairwise_acq %>%
  bf_parameters(prior = Speaker_pairwise_prior_acq) %>%
  arrange(log_BF) # sort according to BF

# add rule-of-thumb interpretation
BF_Speaker_acq <-
  BF_Speaker_acq %>%
  add_column(
    "interpretation" = interpret_bf(
      BF_Speaker_acq$log_BF,
      rules = "raftery1995",
      log = TRUE,
      include_value = TRUE,
      protect_ratio = TRUE,
      exact = TRUE
    ),
    .after = "log_BF"
  )

BF_Speaker_acq
```

We will check whether the effects of Group and TestSpeaker are driven by a certain level of the other two factors with these plots:
```
emmip(model_acq, TestSpeaker ~ Group | sleepState)
emmip(model_acq, Group ~ TestSpeaker | sleepState)
```

### Sensitivity analysis for BFs
As a last step, we will perform a sensitivity analysis for the BFs for each tested effect, to check in how far our BFs are dependent or our estimated effect size in our prior. 

What we will most likely see is that if we assume a very low effect size (SD is low), we have support for H1, but if we assume a very big effect size (SD is very high) we find support for the H0. This is because if we assume a very low effect size (i.e., a narrow prior with low standard deviation), we are essentially saying that we expect the true effect size to be close to zero.  In this case, if the observed data shows an effect that is consistent with this narrow prior, we will likely find support for the alternative hypothesis (H1). On the other hand, if we assume a very high effect size (i.e., a broad prior with high standard deviation), we are indicating that we expect a wide range of possible effect sizes, including very large ones. In this case, if the observed data does not show an effect size as large as the broad prior suggests, the Bayes Factor may favor the null hypothesis (H0), as the data is not providing strong evidence for such a large effect. 

That means that we just need to describe what we see in our sensitivity analyses plots to make sure that our BFs are not solely dependent on our priors.

The code we will use for the sensitivity analysis:
```
prior_sd <- c(1, 5, 8, 10, 15, 20, 30, 40, 50)

# Run the models
for (i in 1:length(prior_sd)) {
  psd <- prior_sd[i]
  fit <- brm(MMR ~ 1 + TestSpeaker * Group + 
               mumDist + 
               nrSpeakersDaily +
               sleepState +
               age +
               (1 + TestSpeaker | Subj),
             data = dat_acq,
             prior = c(
               set_prior("normal(3.5, 20)", class = "Intercept"),
               set_prior(paste0("normal(0,", psd, ")"), class = "b"),
               set_prior("normal(0, 20)", class = "sigma")
             ),
             family = gaussian(), 
             control = list(
               adapt_delta = .99, 
               max_treedepth = 15
             ),
             iter = num_iter, 
             chains = num_chains, 
             warmup = num_warmup,
             thin = num_thin,
             cores = num_chains, 
             seed = project_seed,
             save_pars = save_pars(all = TRUE),
             file=here("data", "sensitivity_analysis", paste0("A5_sensAnal_BF_priorsd_", psd, ".rds"))             
  )
}

### Simple effect Group for speaker 1 -------------------------
BF <- c()
for (i in 1:length(prior_sd)) {
  psd <- prior_sd[i]
  fit <- readRDS(here("data", "sensitivity_analysis", paste0("A5_sensAnal_BF_priorsd_", psd, ".rds")))
  fit_priors <- unupdate(fit)
  
  m_prior <- fit_priors %>%
    emmeans(~ Group | TestSpeaker) %>%
    pairs()
  
  m_post <- fit %>%
    emmeans(~ Group | TestSpeaker) %>%
    pairs()
  
  BF_current <- bf_parameters(m_post, prior = m_prior) %>%
    filter(Parameter == "fam - unfam, 1")
  BF_current <- as.numeric(BF_current)
  
  BF <- c(BF, BF_current)
}

res <- data.frame(prior_sd, BF, logBF = log10(BF))
save(res, file = here("data", "sensitivity_analysis", "A5_sensAnal_BF_simpleGroup-S1.rda"))

breaks <- c(1 / 100, 1 / 50, 1 / 20, 1 / 10, 1 / 3, 1, 3, 5, 10, 20, 50, 100)
ggplot(res, aes(x = prior_sd, y = BF)) +
  geom_point(size = 2) +
  geom_line() +
  geom_hline(yintercept = 1, linetype = "dashed") +
  scale_x_continuous("Normal prior width (SD)\n") +
  scale_y_log10("BF10", breaks = breaks, labels = MASS::fractions(breaks)) +
  coord_cartesian(ylim = c(1 / 100, 100), xlim = c(0, tail(prior_sd, n = 1) + 40)) +
  annotate("text", x = tail(prior_sd, n = 1) + 20, y = 30, label = "Evidence in favor of H1", size = 5) +
  annotate("text", x = tail(prior_sd, n = 1) + 20, y = 1 / 30, label = "Evidence in favor of H0", size = 5) +
  theme(axis.text.y = element_text(size = 8)) +
  ggtitle("Bayes factors for simple effect of Group for Speaker 1")

### Simple effect Group for speaker 2 -------------------------
BF <- c()
for (i in 1:length(prior_sd)) {
  psd <- prior_sd[i]
  fit <- readRDS(here("data", "sensitivity_analysis", paste0("A5_sensAnal_BF_priorsd_", psd, ".rds")))
  fit_priors <- unupdate(fit)
  
  m_prior <- fit_priors %>%
    emmeans(~ Group | TestSpeaker) %>%
    pairs()
  
  m_post <- fit %>%
    emmeans(~ Group | TestSpeaker) %>%
    pairs()
  
  BF_current <- bf_parameters(m_post, prior = m_prior) %>%
    filter(Parameter == "fam - unfam, 2")
  BF_current <- as.numeric(BF_current)
  
  BF <- c(BF, BF_current)
}

res <- data.frame(prior_sd, BF, logBF = log10(BF))
save(res, file = here("data", "sensitivity_analysis", "A5_sensAnal_BF_simpleGroup-S2.rda"))

ggplot(res, aes(x = prior_sd, y = BF)) +
  geom_point(size = 2) +
  geom_line() +
  geom_hline(yintercept = 1, linetype = "dashed") +
  scale_x_continuous("Normal prior width (SD)\n") +
  scale_y_log10("BF10", breaks = breaks, labels = MASS::fractions(breaks)) +
  coord_cartesian(ylim = c(1 / 100, 100), xlim = c(0, tail(prior_sd, n = 1) + 40)) +
  annotate("text", x = tail(prior_sd, n = 1) + 20, y = 30, label = "Evidence in favor of H1", size = 5) +
  annotate("text", x = tail(prior_sd, n = 1) + 20, y = 1 / 30, label = "Evidence in favor of H0", size = 5) +
  theme(axis.text.y = element_text(size = 8)) +
  ggtitle("Bayes factors for simple effect of Group for Speaker 2")

### Effect TestSpeaker-------------------------
BF <- c()
for (i in 1:length(prior_sd)) {
  psd <- prior_sd[i]
  fit <- readRDS(here("data", "sensitivity_analysis", paste0("A5_sensAnal_BF_priorsd_", psd, ".rds")))
  fit_priors <- unupdate(fit)
  
  m_prior <- fit_priors %>%
    emmeans(~ TestSpeaker) %>%
    pairs()
  
  m_post <- fit %>%
    emmeans(~ TestSpeaker) %>%
    pairs()
  
  BF_current <- bf_parameters(m_post, prior = m_prior)
  BF_current <- as.numeric(BF_current)
  
  BF <- c(BF, BF_current)
}

res <- data.frame(prior_sd, BF, logBF = log10(BF))
save(res, file = here("data", "sensitivity_analysis", "A5_sensAnal_BF_testspeaker.rda"))

ggplot(res, aes(x = prior_sd, y = BF)) +
  geom_point(size = 2) +
  geom_line() +
  geom_hline(yintercept = 1, linetype = "dashed") +
  scale_x_continuous("Normal prior width (SD)\n") +
  scale_y_log10("BF10", breaks = breaks, labels = MASS::fractions(breaks)) +
  coord_cartesian(ylim = c(1 / 100, 100), xlim = c(0, tail(prior_sd, n = 1) + 40)) +
  annotate("text", x = tail(prior_sd, n = 1) + 20, y = 30, label = "Evidence in favor of H1", size = 5) +
  annotate("text", x = tail(prior_sd, n = 1) + 20, y = 1 / 30, label = "Evidence in favor of H0", size = 5) +
  theme(axis.text.y = element_text(size = 8)) +
  ggtitle("Bayes factors for effect of TestSpeaker")


### Effect nrSpeakers -------------------------

BF <- c()
for (i in 1:length(prior_sd)) {
  psd <- prior_sd[i]
  fit <- readRDS(here("data", "sensitivity_analysis", paste0("A5_sensAnal_BF_priorsd_", psd, ".rds")))
  fit_priors <- unupdate(fit)
  
  BF_current <- bf_parameters(fit, prior = fit_priors) %>%
    filter(Parameter == "b_nrSpeakersDaily")
  BF_current <- as.numeric(BF_current)
  
  BF <- c(BF, BF_current)
}

res <- data.frame(prior_sd, BF, logBF=log10(BF))
save(res, file=here("data", "sensitivity_analysis","A5_sensAnal_BF_nrSpeakers.rda"))

breaks <- c(1 / 100, 1 / 50, 1 / 20, 1 / 10, 1 / 3, 1, 3, 5, 10, 20, 50, 100)
ggplot(res, aes(x = prior_sd, y = BF)) +
  geom_point(size = 2) +
  geom_line() +
  geom_hline(yintercept = 1, linetype = "dashed") +
  scale_x_continuous("Normal prior width (SD)\n") +
  scale_y_log10("BF10", breaks = breaks, labels = MASS::fractions(breaks)) +
  coord_cartesian(ylim = c(1 / 100, 100), xlim = c(0, tail(prior_sd, n=1) + 40)) +
  annotate("text", x = tail(prior_sd, n=1) + 20, y = 30, label = "Evidence in favor of H1", size = 5) +
  annotate("text", x = tail(prior_sd, n=1) + 20, y = 1 / 30, label = "Evidence in favor of H0", size = 5) +
  theme(axis.text.y = element_text(size = 8)) +
  ggtitle("Bayes factors for effect of nrSpakersDaily")
```




